import pandas as pd
import os

# parse samples metadata
samples_tbl=config["samples"]
samples_meta=pd.read_csv(samples_tbl, header = None)
samples_meta.columns=["Sample", "Path"]
samples_meta=samples_meta.set_index("Sample", drop = False)

# override current working directory
workdir: config["outdir"]

# define pipeline target
rule all:
  input: "{sample}/logs/clean_up.log"

# define input for centrifuge
def centrifuge_input(wildcards):
  sample=wildcards["sample"]
  if config["trim"] == 0:
    return samples_meta.Path[wildcards.sample]
  else:
    return os.path.join(sample, "porechop", sample+"_trimmed.fastq")

rule porechop:
  input:
    reads=lambda wildcards: samples_meta.Path[wildcards.sample]
  threads: 32
  log:
    "{sample}/logs/porechop.log"
  output:
    trimmed_reads=temp("{sample}/porechop/{sample}_trimmed.fastq")
  shell:
    """
    porechop -t {threads} -i {input.reads} -o {output.trimmed_reads}
    """

rule centrifuge:
  input:
    reads=centrifuge_input
  threads: 32
  params:
    db=config["centrifuge_db"]
  log:
    "{sample}/logs/centrifuge.log"
  output:
    centrifuge_res=temp("{sample}/centrifuge/centrifuge_res.tsv"),
    centrifuge_report=temp("{sample}/centrifuge/centrifuge_report.tsv")
  shell:
    """
    centrifuge -p {threads} -U {input.reads} -x {params.db} -S {output.centrifuge_res} \
      --report-file {output.centrifuge_report} -k 50 --ignore-quals
    """

rule partition:
  input:
    centrifuge_res="{sample}/centrifuge/centrifuge_res.tsv",
    reads=lambda wildcards: samples_meta.Path[wildcards.sample]
  threads: 4
  params:
    segment="{segment}",
    ID_mappings=config["pipeline_dir"]+"/database/InfA_refseq_segment_mappings.csv",
    pipeline_dir=config["pipeline_dir"]
  output:
    segment_readIDs=temp("{sample}/centrifuge/{segment}_reads.ids"),
    InfA_reads=temp("{sample}/fastq/{segment}.fastq"),
    target_id=temp("{sample}/target/{segment}_id.txt")
  log:
    "{sample}/logs/partition_{segment}.log"
  shell:
    """
    {params.pipeline_dir}/src/centrifuge_res_process.R {input.centrifuge_res} {params.ID_mappings} {params.segment} {output.target_id} {output.segment_readIDs}
    seqtk subseq {input.reads} {output.segment_readIDs} > {output.InfA_reads}
    """

rule create_target:
  input:
    reads=lambda wildcards: samples_meta.Path[wildcards.sample],
    target_id="{sample}/target/{segment}_id.txt"
  threads: 4
  output:
    target_fastq=temp("{sample}/target/{segment}.fastq"),
    target=temp("{sample}/target/{segment}.fasta")
  shell:
    """
    seqtk subseq {input.reads} {input.target_id} > {output.target_fastq}
    seqtk seq -A {output.target_fastq} > {output.target} 
    """
  
rule racon_1:
  input:
    InfA_reads="{sample}/fastq/{segment}.fastq",
    target="{sample}/target/{segment}.fasta"
  output:
    alignment_1=temp("{sample}/racon/1/InfA_{segment}_alignment.paf"),
    consensus_1="{sample}/racon/1/{segment}.fa"
  log:
    "{sample}/logs/racon_1_{segment}.log"
  threads: 16
  shell:
    """
    minimap2 -x map-ont -t {threads} {input.target} {input.InfA_reads} > {output.alignment_1}
    racon -t {threads} {input.InfA_reads} {output.alignment_1} {input.target} > {output.consensus_1}
    """

rule racon_2:
  input:
    InfA_reads="{sample}/fastq/{segment}.fastq",
    consensus_1="{sample}/racon/1/{segment}.fa"
  output:
    alignment_2=temp("{sample}/racon/2/InfA_{segment}_alignment.paf"),
    consensus_2="{sample}/racon/2/{segment}.fa"
  log:
    "{sample}/logs/racon_2_{segment}.log"
  threads: 16
  shell:
    """
    minimap2 -x map-ont -t {threads} {input.consensus_1} {input.InfA_reads} > {output.alignment_2}
    racon -t {threads} {input.InfA_reads} {output.alignment_2} {input.consensus_1} > {output.consensus_2}
    """

rule medaka:
  input:
    InfA_reads="{sample}/fastq/{segment}.fastq",
    consensus_4="{sample}/racon/2/{segment}.fa"
  threads: 16
  params:
    outdir="{sample}/medaka/{segment}"
  log:
    "{sample}/logs/medaka_{segment}.log"
  output:
    consensus="{sample}/consensus/{segment}.fa"
  shell:
    """
    medaka_consensus -i {input.InfA_reads} -d {input.consensus_4} -o {params.outdir} -t 4
    cp {params.outdir}/consensus.fasta {output.consensus}
    """

rule clean:
  input:
    consensus=expand("{sample}/consensus/{segment}.fa", sample=samples_meta.Sample, segment=["segment_" + x for x in [str(s) for s in list(range(1,9))]]),
    fastq_dir=temp("{sample}/fastq"),
    racon_dir=temp("{sample}/racon"),
    medaka_dir=temp("{sample}/medaka"),
    centrifuge_dir=temp("{sample}/centrifuge"),
    target_dir=temp("{sample}/target")
  output:
    log="{sample}/logs/clean_up.log"
  shell:
    """
    echo "Pipeline successfully ran to completion" > {output.log}
    """
  